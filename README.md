# Machine-Learning-Engineer-internship-Technocolabs





## Table of content

1. [Application](#Application)




# Application

I have applied for the machine learning engineer intern at Technocolabs, thanks to the creators of this repository: <br>

https://github.com/ACM-Alexandria-SC/Internships <br>

This repository lists all available internships remotely and in-site. 

## Interview process 

After applying, at 31 of December, 2022, I have received an email at 2nd of janurary, 2023 from Mohammed Yassin Shah, to conduct an interview, which contains number of questions:
1. Talk about yourself
2. Do you have any previous internship experience
3. Why do you want to join technocolabs as ML intern
4. Say you scrap the data from different sources about funding loans, and you found number of null rows, what will you do. Say the null values were 50% or less, how about 80% and above, how can we deal with these null values?
6. How can we convert numerical data to categorcal data and vice virca
7. How can we find a correlationship 
8. How can we deal with outliers
9. What is features engineering
10. What supervised ML algorithms do you know


Then we speaks about the internship process:

## Internship process 

Technocolabs is a multinational company that takes data from number of diffrent sources, and apply data analysis and machine learning models to 
the data of the customers, then they apply these findings into their web and mobile app business. The interns will be monitored by the company's project manager and mentor. There will be weekly meeting of which we discuss the data and the requirements. The meeting will last for an hour, and it's conducted in the weekend. If you missed the meeting, there will be confusion of term of what the requirements are. There will be (10) interns whom they will  work in groups, and there is weekly submission. Once you got shortlisted, you have to pay 30.00 USD registration fees. The result if whither you got shortlisted will be sent to your email in about half and hour, so check you spam folder, and make sure to confirm the payment in the next 3 hours.

## Internship tasks


#### Data scraping
There will be data scraping from different sources, using ADE which stands for Automated Data Extraction. It is a process or technique that involves using software or other automated means to extract data from a variety of sources, such as websites, documents, or databases. Automated data extraction can be used to quickly and accurately gather large amounts of data, which can then be used for a variety of purposes, such as data analysis, data visualization, or machine learning.


#### Data processing 
Data cleaning, data mantaining

#### Descriptive analysis 
Create categorcal and labels, encoding data.

#### Data modeling
Based on the problem statment, we choose the data modeling:
1. Reggrition
2. Classification 
3. Clusturing
Based on the highest accuracy, we will stick with the data modeling 

#### Deploy the data

After that we will deploy the data in the cloud, either in IBM watson cloud, Herouko, or Robust

| Task         | Description|
| :-------------: | :-------------: |
| BigMart Prediction Model | https://shahyaseen71.gitbook.io/technocolabs-mini-project/ |
| Bandora Data Preprocessing and Encoding | https://technocollabs.gitbook.io/bondora-statistics/ |
| EDA(Exploratory Data Analysis) | Whether the product is low fat or not|






## Learning material

I received an email containing the pre

https://technocolabs-internship.gitbook.io/internship-prerequisites-learning-resources/

https://www.analyticsvidhya.com/blog/2021/08/how-to-perform-exploratory-data-analysis-a-guide-for-beginners/


 https://towardsdatascience.com/exploratory-data-analysis-dcb5e7189c4e


 https://medium.com/analytics-vidhya/exploratory-data-analysis-for-beginner-7488d587f1ec
 
 
 # Project-Bondora-Financial-risk-modelling-of-European-P2P-lending-platform
The main purposes of this analysis are to summarize the characteristics of variables that can affect the loan status and to get some ideas about the relationships among variables.


This project is a collaboration between our team that worked on developing a machine learning model using Gradient Boosting and VSM (Vector Space Model). We aimed to achieve the highest accuracy in our model by comparing our data and performing different EDA (Exploratory Data Analysis) techniques on our target variable.

We started by analyzing three different datasets, each with its own set of features and target variables. Ashis's data had 36 columns, Chebrolu's had 27 columns, and my data had 14 columns. We performed Gradient Boosting and SVM on each dataset and compared the accuracy of our models.

After a thorough analysis, we found that my data had the highest accuracy in the Gradient Boosting model with a value of 1.0, while Chebrolu's accuracy was 0.1 and Ashis's accuracy was 0.6. In the SVM model, my data had an accuracy of 0.47. Based on these results, we decided to use my data for our final model, as it had the most accurate results in the Gradient Boosting model.

Here is a breakdown of the roles and responsibilities of each team member:


| Team member name         | Role and Tasks|
| :-------------: | :-------------: |
| Fatimah | Data preprocessing feature engineering and modeling |
| Chebrolu | Data cleaning and preprocessing, and feature engineering  |
| Ashish | Data preprocessing, feature engineering and modeling |
| Asif | Data analysis and preprocessing |
| Mohamed | Data analysis and preprocessing |


## Modeling

### Gradient Boosting classifier

<img width="410" alt="image" src="https://user-images.githubusercontent.com/123512564/221941696-afda8083-3ef1-4430-85dd-c3b2b036e533.png">


### Super vector machine model


<img width="407" alt="image" src="https://user-images.githubusercontent.com/123512564/221941466-3125d410-f054-4ee5-ae1c-fbcba4d1705e.png">








